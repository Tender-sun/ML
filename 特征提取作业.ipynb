{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "特征提取作业.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1h_Vtsxg7Te6eHLYGAk-ygytncQdWlOeI",
      "authorship_tag": "ABX9TyNMhB4rGoBVEWwsQvZAS4Pr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tender-sun/ML/blob/main/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%BD%9C%E4%B8%9A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzsDoVYKGoYr"
      },
      "source": [
        "import numpy as np\n",
        "import pdb\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from matplotlib.image import imread\n",
        "\n",
        "def tidy_plot(xmin, xmax, ymin, ymax, center = False, title = None,\n",
        "                 xlabel = None, ylabel = None):\n",
        "    plt.ion()\n",
        "    plt.figure(facecolor=\"white\")\n",
        "    ax = plt.subplot()\n",
        "    if center:\n",
        "        ax.spines['left'].set_position('zero')\n",
        "        ax.spines['right'].set_color('none')\n",
        "        ax.spines['bottom'].set_position('zero')\n",
        "        ax.spines['top'].set_color('none')\n",
        "        ax.spines['left'].set_smart_bounds(True)\n",
        "        ax.spines['bottom'].set_smart_bounds(True)\n",
        "        ax.xaxis.set_ticks_position('bottom')\n",
        "        ax.yaxis.set_ticks_position('left')\n",
        "    else:\n",
        "        ax.spines[\"top\"].set_visible(False)\n",
        "        ax.spines[\"right\"].set_visible(False)\n",
        "        ax.get_xaxis().tick_bottom()\n",
        "        ax.get_yaxis().tick_left()\n",
        "    eps = .05\n",
        "    plt.xlim(xmin-eps, xmax+eps)\n",
        "    plt.ylim(ymin-eps, ymax+eps)\n",
        "    if title: ax.set_title(title)\n",
        "    if xlabel: ax.set_xlabel(xlabel)\n",
        "    if ylabel: ax.set_ylabel(ylabel)\n",
        "    return ax\n",
        "\n",
        "def plot_separator(ax, th, th_0):\n",
        "    xmin, xmax = ax.get_xlim()\n",
        "    ymin,ymax = ax.get_ylim()\n",
        "    pts = []\n",
        "    eps = 1.0e-6\n",
        "    # xmin boundary crossing is when xmin th[0] + y th[1] + th_0 = 0\n",
        "    # that is, y = (-th_0 - xmin th[0]) / th[1]\n",
        "    if abs(th[1,0]) > eps:\n",
        "        pts += [np.array([x, (-th_0 - x * th[0,0]) / th[1,0]]) \\\n",
        "                                                        for x in (xmin, xmax)]\n",
        "    if abs(th[0,0]) > 1.0e-6:\n",
        "        pts += [np.array([(-th_0 - y * th[1,0]) / th[0,0], y]) \\\n",
        "                                                         for y in (ymin, ymax)]\n",
        "    in_pts = []\n",
        "    for p in pts:\n",
        "        if (xmin-eps) <= p[0] <= (xmax+eps) and \\\n",
        "           (ymin-eps) <= p[1] <= (ymax+eps):\n",
        "            duplicate = False\n",
        "            for p1 in in_pts:\n",
        "                if np.max(np.abs(p - p1)) < 1.0e-6:\n",
        "                    duplicate = True\n",
        "            if not duplicate:\n",
        "                in_pts.append(p)\n",
        "    if in_pts and len(in_pts) >= 2:\n",
        "        # Plot separator\n",
        "        vpts = np.vstack(in_pts)\n",
        "        ax.plot(vpts[:,0], vpts[:,1], 'k-', lw=2)\n",
        "        # Plot normal\n",
        "        vmid = 0.5*(in_pts[0] + in_pts[1])\n",
        "        scale = np.sum(th*th)**0.5\n",
        "        diff = in_pts[0] - in_pts[1]\n",
        "        dist = max(xmax-xmin, ymax-ymin)\n",
        "        vnrm = vmid + (dist/10)*(th.T[0]/scale)\n",
        "        vpts = np.vstack([vmid, vnrm])\n",
        "        ax.plot(vpts[:,0], vpts[:,1], 'k-', lw=2)\n",
        "        # Try to keep limits from moving around\n",
        "        ax.set_xlim((xmin, xmax))\n",
        "        ax.set_ylim((ymin, ymax))\n",
        "    else:\n",
        "        print('Separator not in plot range')\n",
        "\n",
        "def plot_data(data, labels, ax = None, clear = False,\n",
        "                  xmin = None, xmax = None, ymin = None, ymax = None):\n",
        "    if ax is None:\n",
        "        if xmin == None: xmin = np.min(data[0, :]) - 0.5\n",
        "        if xmax == None: xmax = np.max(data[0, :]) + 0.5\n",
        "        if ymin == None: ymin = np.min(data[1, :]) - 0.5\n",
        "        if ymax == None: ymax = np.max(data[1, :]) + 0.5\n",
        "        ax = tidy_plot(xmin, xmax, ymin, ymax)\n",
        "\n",
        "        x_range = xmax - xmin; y_range = ymax - ymin\n",
        "        if .1 < x_range / y_range < 10:\n",
        "            ax.set_aspect('equal')\n",
        "        xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
        "    elif clear:\n",
        "        xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
        "        ax.clear()\n",
        "    else:\n",
        "        xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
        "    colors = np.choose(labels > 0, cv(['r', 'g']))[0]\n",
        "    ax.scatter(data[0,:], data[1,:], c = colors,\n",
        "                    marker = 'o', s=50, edgecolors = 'none')\n",
        "    # Seems to occasionally mess up the limits\n",
        "    ax.set_xlim(xlim); ax.set_ylim(ylim)\n",
        "    ax.grid(True, which='both')\n",
        "    #ax.axhline(y=0, color='k')\n",
        "    #ax.axvline(x=0, color='k')\n",
        "    return ax\n",
        "\n",
        "######################################################################\n",
        "#   Utilities\n",
        "\n",
        "# Takes a list of numbers and returns a column vector:  n x 1\n",
        "def cv(value_list):\n",
        "    return np.transpose(rv(value_list))\n",
        "\n",
        "# Takes a list of numbers and returns a row vector: 1 x n\n",
        "def rv(value_list):\n",
        "    return np.array([value_list])\n",
        "\n",
        "# x is dimension d by n\n",
        "# th is dimension d by m\n",
        "# th0 is dimension 1 by m\n",
        "# return matrix of y values for each column of x and theta: dimension m x n\n",
        "def y(x, th, th0):\n",
        "    return np.dot(np.transpose(th), x) + np.transpose(th0)\n",
        "\n",
        "def length(d_by_m):\n",
        "    return np.sum(d_by_m * d_by_m, axis = 0, keepdims = True)**0.5\n",
        "\n",
        "# x is dimension d by n\n",
        "# th is dimension d by m\n",
        "# th0 is dimension 1 by m\n",
        "# return matrix of signed dist for each column of x and theta: dimension m x n\n",
        "def signed_dist(x, th, th0):\n",
        "    return y(x, th, th0) / np.transpose(length(th))\n",
        "######################################################################\n",
        "# Perceptron code\n",
        "\n",
        "# data is dimension d by n\n",
        "# labels is dimension 1 by n\n",
        "# T is a positive integer number of steps to run\n",
        "# Perceptron algorithm with offset.\n",
        "# data is dimension d by n\n",
        "# labels is dimension 1 by n\n",
        "# T is a positive integer number of steps to run\n",
        "def perceptron(data, labels, params = {}, hook = None):\n",
        "    # if T not in params, default to 50\n",
        "    T = params.get('T', 50)\n",
        "    (d, n) = data.shape\n",
        "\n",
        "    theta = np.zeros((d, 1)); theta_0 = np.zeros((1, 1))\n",
        "    for t in range(T):\n",
        "        for i in range(n):\n",
        "            x = data[:,i:i+1]\n",
        "            y = labels[:,i:i+1]\n",
        "            if y * positive(x, theta, theta_0) <= 0.0:\n",
        "                theta = theta + y * x\n",
        "                theta_0 = theta_0 + y\n",
        "                if hook: hook((theta, theta_0))\n",
        "    return theta, theta_0\n",
        "\n",
        "def averaged_perceptron(data, labels, params = {}, hook = None):\n",
        "    T = params.get('T', 100)\n",
        "    (d, n) = data.shape\n",
        "    theta = np.zeros((d, 1)); theta_0 = np.zeros((1, 1))\n",
        "    theta_sum = theta.copy()\n",
        "    theta_0_sum = theta_0.copy()\n",
        "    for t in range(T):\n",
        "        for i in range(n):\n",
        "            x = data[:,i:i+1]\n",
        "            y = labels[:,i:i+1]\n",
        "            if y * positive(x, theta, theta_0) <= 0.0:\n",
        "                theta = theta + y * x\n",
        "                theta_0 = theta_0 + y\n",
        "                if hook: hook((theta, theta_0))\n",
        "            theta_sum = theta_sum + theta\n",
        "            theta_0_sum = theta_0_sum + theta_0\n",
        "    theta_avg = theta_sum / (T*n)\n",
        "    theta_0_avg = theta_0_sum / (T*n)\n",
        "    if hook: hook((theta_avg, theta_0_avg))\n",
        "    return theta_avg, theta_0_avg\n",
        "\n",
        "def positive(x, th, th0):\n",
        "    return np.sign(th.T@x + th0)\n",
        "\n",
        "def score(data, labels, th, th0):\n",
        "    return np.sum(positive(data, th, th0) == labels)\n",
        "\n",
        "def eval_classifier(learner, data_train, labels_train, data_test, labels_test):\n",
        "    th, th0 = learner(data_train, labels_train)\n",
        "    return score(data_test, labels_test, th, th0)/data_test.shape[1]\n",
        "\n",
        "def xval_learning_alg(learner, data, labels, k):\n",
        "    _, n = data.shape\n",
        "    idx = list(range(n))\n",
        "    np.random.seed(0)\n",
        "    np.random.shuffle(idx)\n",
        "    data, labels = data[:,idx], labels[:,idx]\n",
        "\n",
        "    s_data = np.array_split(data, k, axis=1)\n",
        "    s_labels = np.array_split(labels, k, axis=1)\n",
        "\n",
        "    score_sum = 0\n",
        "    for i in range(k):\n",
        "        data_train = np.concatenate(s_data[:i] + s_data[i+1:], axis=1)\n",
        "        labels_train = np.concatenate(s_labels[:i] + s_labels[i+1:], axis=1)\n",
        "        data_test = np.array(s_data[i])\n",
        "        labels_test = np.array(s_labels[i])\n",
        "        score_sum += eval_classifier(learner, data_train, labels_train,\n",
        "                                              data_test, labels_test)\n",
        "    return score_sum/k\n",
        "\n",
        "######################################################################\n",
        "#   Tests\n",
        "\n",
        "def test_linear_classifier(dataFun, learner, learner_params = {},\n",
        "                             draw = True, refresh = True, pause = True):\n",
        "    data, labels = dataFun()\n",
        "    d, n = data.shape\n",
        "    if draw:\n",
        "        ax = plot_data(data, labels)\n",
        "        def hook(params):\n",
        "            (th, th0) = params\n",
        "            if refresh: plot_data(data, labels, ax, clear = True)\n",
        "            plot_separator(ax, th, th0)\n",
        "            print('th', th.T, 'th0', th0)\n",
        "            if pause: input('go?')\n",
        "    else:\n",
        "        hook = None\n",
        "    th, th0 = learner(data, labels, hook = hook, params = learner_params)\n",
        "    print(\"Final score\", float(score(data, labels, th, th0)) / n)\n",
        "    print(\"Params\", np.transpose(th), th0)\n",
        "\n",
        "######################################################################\n",
        "# For auto dataset\n",
        "\n",
        "def load_auto_data(path_data):\n",
        "    \"\"\"\n",
        "    Returns a list of dict with keys:\n",
        "    \"\"\"\n",
        "    numeric_fields = {'mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
        "                      'acceleration', 'model_year', 'origin'}\n",
        "    data = []\n",
        "    with open(path_data) as f_data:\n",
        "        for datum in csv.DictReader(f_data, delimiter='\\t'):\n",
        "            for field in list(datum.keys()):\n",
        "                if field in numeric_fields and datum[field]:\n",
        "                    datum[field] = float(datum[field])\n",
        "            data.append(datum)\n",
        "    return data\n",
        "\n",
        "# Feature transformations\n",
        "def std_vals(data, f):\n",
        "    vals = [entry[f] for entry in data]\n",
        "    avg = sum(vals)/len(vals)\n",
        "    dev = [(entry[f] - avg)**2 for entry in data]\n",
        "    sd = (sum(dev)/len(vals))**0.5\n",
        "    return (avg, sd)\n",
        "\n",
        "def standard(v, std):\n",
        "    return [(v-std[0])/std[1]]\n",
        "\n",
        "def raw(x):\n",
        "    return [x]\n",
        "\n",
        "def one_hot(v, entries):\n",
        "    vec = len(entries)*[0]\n",
        "    vec[entries.index(v)] = 1\n",
        "    return vec\n",
        "\n",
        "# The class (mpg) added to the front of features\n",
        "def auto_data_and_labels(auto_data, features):\n",
        "    features = [('mpg', raw)] + features\n",
        "    std = {f:std_vals(auto_data, f) for (f, phi) in features if phi==standard}\n",
        "    entries = {f:list(set([entry[f] for entry in auto_data])) \\\n",
        "               for (f, phi) in features if phi==one_hot}\n",
        "    print('avg and std', std)\n",
        "    print('entries in one_hot field',   entries)\n",
        "    vals = []\n",
        "    for entry in auto_data:\n",
        "        phis = []\n",
        "        for (f, phi) in features:\n",
        "            if phi == standard:\n",
        "                phis.extend(phi(entry[f], std[f]))\n",
        "            elif phi == one_hot:\n",
        "                phis.extend(phi(entry[f], entries[f]))\n",
        "            else:\n",
        "                phis.extend(phi(entry[f]))\n",
        "        vals.append(np.array([phis]))\n",
        "    data_labels = np.vstack(vals)\n",
        "    return data_labels[:, 1:].T, data_labels[:, 0:1].T\n",
        "\n",
        "######################################################################\n",
        "# For food review dataset\n",
        "\n",
        "from string import punctuation, digits, printable\n",
        "import csv\n",
        "\n",
        "def load_review_data(path_data):\n",
        "    \"\"\"\n",
        "    Returns a list of dict with keys:\n",
        "    * sentiment: +1 or -1 if the review was positive or negative, respectively\n",
        "    * text: the text of the review\n",
        "    \"\"\"\n",
        "    basic_fields = {'sentiment', 'text'}\n",
        "    data = []\n",
        "    with open(path_data) as f_data:\n",
        "        for datum in csv.DictReader(f_data, delimiter='\\t'):\n",
        "            for field in list(datum.keys()):\n",
        "                if field not in basic_fields:\n",
        "                    del datum[field]\n",
        "            if datum['sentiment']:\n",
        "                datum['sentiment'] = int(datum['sentiment'])\n",
        "            data.append(datum)\n",
        "    return data\n",
        "\n",
        "printable = set(printable)\n",
        "def clean(s):\n",
        "    return filter(lambda x: x in printable, s)\n",
        "\n",
        "def extract_words(input_string):\n",
        "    \"\"\"\n",
        "    Helper function for bag_of_words()\n",
        "    Inputs a text string\n",
        "    Returns a list of lowercase words in the string.\n",
        "    Punctuation and digits are separated out into their own words.\n",
        "    \"\"\"\n",
        "    for c in punctuation + digits:\n",
        "        input_string = input_string.replace(c, ' ' + c + ' ')\n",
        "\n",
        "    # return [ps.stem(w) for w in input_string.lower().split()]\n",
        "    return input_string.lower().split()\n",
        "\n",
        "def bag_of_words(texts):\n",
        "    \"\"\"\n",
        "    Inputs a list of string reviews\n",
        "    Returns a dictionary of unique unigrams occurring over the input\n",
        "\n",
        "    Feel free to change this code as guided by Section 3 (e.g. remove stopwords, add bigrams etc.)\n",
        "    \"\"\"\n",
        "    dictionary = {} # maps word to unique index\n",
        "    for text in texts:\n",
        "        word_list = extract_words(text)\n",
        "        for word in word_list:\n",
        "            if word not in dictionary:\n",
        "                dictionary[word] = len(dictionary)\n",
        "    return dictionary\n",
        "\n",
        "def extract_bow_feature_vectors(reviews, dictionary):\n",
        "    \"\"\"\n",
        "    Inputs a list of string reviews\n",
        "    Inputs the dictionary of words as given by bag_of_words\n",
        "    Returns the bag-of-words feature matrix representation of the data.\n",
        "    The returned matrix is of shape (n, m), where n is the number of reviews\n",
        "    and m the total number of entries in the dictionary.\n",
        "    \"\"\"\n",
        "\n",
        "    num_reviews = len(reviews)\n",
        "    feature_matrix = np.zeros([num_reviews, len(dictionary)])\n",
        "\n",
        "    for i, text in enumerate(reviews):\n",
        "        word_list = extract_words(text)\n",
        "        for word in word_list:\n",
        "            if word in dictionary:\n",
        "                feature_matrix[i, dictionary[word]] = 1\n",
        "    # We want the feature vectors as columns\n",
        "    return feature_matrix.T\n",
        "\n",
        "\n",
        "def reverse_dict(d):\n",
        "    return {v: k for k, v in d.items()}\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# For MNIST dataset\n",
        "\n",
        "\n",
        "# NOTE you should use this function to evaluate your MNIST results!\n",
        "def get_classification_accuracy(data, labels):\n",
        "    \"\"\"\n",
        "    @param data (d,n) array\n",
        "    @param labels (1,n) array\n",
        "    \"\"\"\n",
        "    return xval_learning_alg(lambda data, labels: perceptron(data, labels, {\"T\": 50}), data, labels, 10)\n",
        "\n",
        "\n",
        "def load_mnist_data(labels):\n",
        "    \"\"\"\n",
        "    @param labels list of labels from {0, 1,...,9}\n",
        "    @return dict: label (int) -> [[image1], [image2], ...]\n",
        "    \"\"\"\n",
        "\n",
        "    data = {}\n",
        "\n",
        "    for label in labels:\n",
        "        images = load_mnist_single(\"mnist/mnist_train{}.png\".format(label))\n",
        "        y = np.array([[label] * len(images)])\n",
        "        data[label] = {\n",
        "            \"images\": images,\n",
        "            \"labels\": y\n",
        "        }\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_mnist_single(path_data):\n",
        "    \"\"\"\n",
        "    @return list of images (first row of large picture)\n",
        "    \"\"\"\n",
        "\n",
        "    img = imread(path_data)  # 2156 x 2156 (m,n)\n",
        "    m, n = img.shape\n",
        "\n",
        "    side_len = 28  # standard mnist\n",
        "    n_img = int(m / 28)\n",
        "\n",
        "    imgs = []  # list of single images\n",
        "    for i in range(n_img):\n",
        "        start_ind = i*side_len\n",
        "        end_ind = start_ind + side_len\n",
        "        current_img = img[start_ind:end_ind, :side_len]  # 28 by 28\n",
        "\n",
        "        current_img = current_img / 255 # normalization!!!\n",
        "\n",
        "        imgs.append(current_img)\n",
        "\n",
        "    return imgs\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTQ40-75LV9n"
      },
      "source": [
        "## 下面是作业实现的代码"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "Y1IB5ALXLflL",
        "outputId": "0abe0d42-d149-4c91-be5a-33be8fc8d164"
      },
      "source": [
        "path_data = './auto-mpg.tsv'\n",
        "#加载数据\n",
        "auto_data_all = load_auto_data(path_data)\n",
        "features = [('cylinders', raw),\n",
        "            ('displacement', raw),\n",
        "            ('horsepower', raw),\n",
        "            ('weight', raw),\n",
        "            ('acceleration', raw),\n",
        "            ('origin', raw)]\n",
        "auto_data, auto_labels = auto_data_and_labels(auto_data_all, features)\n",
        "t1 = xval_learning_alg(lambda data, labels: perceptron(data,labels,{\"T\" : 1}) ,auto_data,auto_labels,10)\n",
        "t2 = xval_learning_alg(lambda data, labels: averaged_perceptron(data,labels,{\"T\" : 1}) ,auto_data,auto_labels,10)\n",
        "t3 = xval_learning_alg(lambda data, labels: perceptron(data,labels,{\"T\" : 10}) ,auto_data,auto_labels,10)\n",
        "t4 = xval_learning_alg(lambda data, labels: averaged_perceptron(data,labels,{\"T\" : 10}) ,auto_data,auto_labels,10)\n",
        "t5 = xval_learning_alg(lambda data, labels: perceptron(data,labels,{\"T\" : 50}) ,auto_data,auto_labels,10)\n",
        "t6 = xval_learning_alg(lambda data, labels: averaged_perceptron(data,labels,{\"T\" : 50}) ,auto_data,auto_labels,10)\n",
        "      \n",
        "features_2 = [('cylinders', one_hot),\n",
        "            ('displacement', standard),\n",
        "            ('horsepower', standard),\n",
        "            ('weight', standard),\n",
        "            ('acceleration', standard),\n",
        "            ('origin', one_hot)]\n",
        "auto_data2, auto_labels2 = auto_data_and_labels(auto_data_all, features_2)\n",
        "c1 = xval_learning_alg(lambda data, labels: perceptron(data,labels,{\"T\" : 1}) ,auto_data2,auto_labels2,10)\n",
        "c2 = xval_learning_alg(lambda data, labels: averaged_perceptron(data,labels,{\"T\" : 1}) ,auto_data2,auto_labels2,10)\n",
        "c3 = xval_learning_alg(lambda data, labels: perceptron(data,labels,{\"T\" : 10}) ,auto_data2,auto_labels2,10)\n",
        "c4 = xval_learning_alg(lambda data, labels: averaged_perceptron(data,labels,{\"T\" : 10}) ,auto_data2,auto_labels2,10)\n",
        "c5 = xval_learning_alg(lambda data, labels: perceptron(data,labels,{\"T\" : 50}) ,auto_data2,auto_labels2,10)\n",
        "c6 = xval_learning_alg(lambda data, labels: averaged_perceptron(data,labels,{\"T\" : 50}) ,auto_data2,auto_labels2,10)\n",
        "print(\"特征1，T=1的精度分别为感知机(%s)和平均感知机(%s)\"%(t1,t2))\n",
        "print(\"特征1，T=10的精度分别为感知机(%s)和平均感知机(%s)\"%(t3,t4))\n",
        "print(\"特征1，T=50的精度分别为感知机(%s)和平均感知机(%s)\"%(t5,t6))\n",
        "print(\"特征2，T=1的精度分别为感知机(%s)和平均感知机(%s)\"%(c1,c2))\n",
        "print(\"特征2，T=10的精度分别为感知机(%s)和平均感知机(%s)\"%(c3,c4))\n",
        "print(\"特征2，T=50的精度分别为感知机(%s)和平均感知机(%s)\"%(c5,c6))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-07fa925a1c24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/raw.githubusercontent.com/Tender-sun/ML/main/data/auto-mpg.tsv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#加载数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mauto_data_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_auto_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m features = [('cylinders', raw),\n\u001b[1;32m      5\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m'displacement'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-70ea4db83643>\u001b[0m in \u001b[0;36mload_auto_data\u001b[0;34m(path_data)\u001b[0m\n\u001b[1;32m    233\u001b[0m                       'acceleration', 'model_year', 'origin'}\n\u001b[1;32m    234\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdatum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/raw.githubusercontent.com/Tender-sun/ML/main/data/auto-mpg.tsv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc2j0ygqo91n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}